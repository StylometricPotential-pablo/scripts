This literature review attempts to summarize the scientific literature surrounding the risk of bias in clinical machine learning systems, and relevant solutions. This has been done by analyzing relevant scientific research and discussing the pre-eminent scientific opinions. Upon inspection of this research, it becomes clear that bias in AI poses a threat to the equality of the medical landscape, and measures must be put in place in order to promote equity in the medical landscape. By analyzing these issues, this literature review presents the largest issues and potential solutions to the bias problem in machine learning; we must recognize the dearth of accurate training data as well as distribution shift and other processes of machine learning bias, in order to realize a future with unbiased and effective machine learning healthcare algorithms.
Keywords: distribution shift, ethno-racial equality, machine learning healthcare algorithms, black-box.

	The prevalence of machine learning systems in the modern world is unquestionable; popular use cases like aviation, Google Maps, corporate tools (NYCC, 2021), and weather forecasting (Bagaric, 2022) are important for everyday existence in our modern society. One such industry which has begun to rely heavily on artificial intelligence is healthcare: from triaging patients to assisting in diagnoses, doctors have found incredible uses in these advanced computer system technologies.  The possibilities for these technological advancements are numerous: researchers have proposed future AI solutions in the healthcare space including abilities to perform pre-diagnostic routines, automatically screening patients, and referrals (Challen et al., 2019). However, the adoption of these technologies has exposed a large issue: artificial intelligence has a strong tendency to create and spread bias.  Potential and current risks from AI include disproportionately misdiagnosing minorities and an increased proclivity to treat specific groups of people ineffectively (Obermeyer et al., 2019). As machine learning systems advance, researchers believe this issue will only grow larger, and so they have proposed various solutions and frameworks for avoiding the many possible pitfalls of AI in healthcare. This literature review considers the future of machine learning in healthcare, taking into account its biased qualities, by responding to the following questions: 
What are the roles of AI systems which necessitate unbiasedness in clinical settings and beyond?
How does and will bias in AI systems impact industries and issues of inequality in the US?
What is being done and should be done to fix the biases prevalent in machine learning systems?
Artificial intelligence systems have begun to transform and accelerate innovation in a broad array of fields, but the literature suggests that this nascent technology, if unchecked, will develop biases which put many communities at risk, especially in the context of healthcare. This literature review will discuss the advantages and drawbacks of AI in healthcare, while going over proposed solutions to the bias problem.
	The scientific literature has performed a large amount of research into the functioning of these machine learning algorithms, which is essential for our understanding of the resultant bias problem. Opacity is defined as the uncertainty that intrinsically surrounds these systems (Burrell et al., 2016) and highlights the property of machine learning that causes bias to be so impactful. Artificial intelligence inherently works as a black box; we do not know exactly why it performs one task over another.  These algorithms simply take data in, and then produce an output, after having been trained on vast amounts of predefined data; in their training, they fine tune their parameters with the goal of producing an output which most closely resembles the objective they have been given. (Erikson et al., 2017) 
	These machine learning techniques have a never-before-seen potential to change the workings of our society at large, as well as specifically the medical field. Since the 1990s, medical researchers have realized the tremendous abilities that machine learning systems have in clinical settings; they pointed towards applications in biomedical signal processing, where there are too many parameters at play for the human doctors typical intellectual processing, or to potential medical image interpretation systems, where AIs could reduce the need for invasive procedures by reaching accurate conclusions off of data that would otherwise be insufficient. (Magoulas et al., 2001).  More modern researchers have realized the potential for accurate risk prediction and diagnosing, potentially saving millions from diseases that would have otherwise gone unnoticed (Challen et al., 2019), while also preventing excessive treatment from misdiagnoses which worsens the lives of many (Sohlberg et al., 2019). Even more forward-looking researchers have suggested that doctors will one day be mostly replaced by intelligent systems, with triaging of patients (Challen et al., 2019) and even surgery predicted to be handled by machine learning systems much more effectively than human doctors. (Garrow et al., 2020) Medical practices are already accepting the revolutionary technologies of machine learning; medical imaging above all other fields has benefited from the advanced technologies of machine learning, with machines proving to be much more effective than doctors at identifying signs of illness (Erickson et al., 2017)
	Virtually all medical professionals agree that machine learning systems will continue to have greater and greater impacts on their practices, as research indicates that AI systems will be much more efficient and accurate than human professionals. This raises the need for assurances of safety, but the issue of fairness and bias remains an unsolved issue. (Bagaric et al., 2022;  Kasapoğlu et al., 2019).
	In recent years, researchers and medical professionals have dedicated significant time to analyzing and classifying the mechanisms of bias which plague the nascent field of machine learning in healthcare, from natural AI processes to academia-specific issues.
	The clearest example of machine learning bias resides in the programmers who code them themselves. Any algorithm works according to the design that a computer scientist makes. If that programmer programs with biases they have inherited through their upbringing or societal factors, according to (Nelson, 2019), that algorithm will undoubtedly carry those biases along with theml. Several researches have posited that we must establish what it means to be fair in order to expunge the bias that exists in these systems, but regardless, machine learning algorithms will, if left unchecked, carry the same biases that permeate the medical space, but perhaps with more disastrous consequences.
	In speaking of bias in AI, researchers emphasize the importance of a concept known as distribution shift. Distribution shift is what occurs when an algorithm’s training and real world data do not correspond well enough. A simple example of this would be a machine learning algorithm being trained to recognize signs of cancer using exclusively data from white American patients; in the real world, it would come across patients of all ethnicities and skin colors. In such a situation, what may seem like a highly accurate algorithm (on the training data) could misdiagnose many of its so-called “out-of-sample” real world patients (Guo et al., 2021). While this example is easy to understand, more complex cases of distribution shift have arisen; if the general population’s risk profile or overall medical background is just slightly different than the data that an algorithm is trained on, your algorithm may fail completely (Challen et al., 20). The inherent “black-box” nature of machine learning algorithms adds a level of complexity for researchers attempting to fix or analyze machine learning systems; their outputs are by design arrived at through an inscrutable process.
	Distribution shift is an inevitable phenomenon in the ever changing field of medicine, but it highlights the importance of high-quality and diverse data sets. A high enough quality dataset can ameliorate the issues of distribution shift. Yet, many researchers have pointed out the dearth of accurately labeled and sufficiently diverse medical datasets (Christensen et al., 2021). In fact, in assessing a huge array of PubMed articles, researchers have  found that 40 percent of databases came from the US and 13.7 percent from China, with first/last authors being overwhelmingly male statisticians as opposed to non-male or clinicians.   Among the few articles which did report racial data, 69.5% of the data came from white patients, 17.1% from Black, and 3.7% Asian (Celi, 2022). Indeed, Native American patient data was only present in two of the articles surveyed. These lacking datasets combined with machine learning’s propensity to pick up biases in its data lead to grim conclusions regarding the efficacy of these machine learning systems.
	This bias is largely theoretical, but we have seen its real impact on communities. For example,  (Obermeyer et al., 2019), published in Science, concluded that algorithms were less likely to recommend care to black communities with the same level of need. The research suggests that if medical professionals do not face the issue of bias in these intelligent systems, more communities will be adversely impacted by this developing technology.
	Researchers have many wide-ranging ideas on how to fix the bias problem in healthcare: solutions range from screening training data to radical transparency.
An important context for the analysis of this literature is the definition of fairness. Algorithmic fairness is the idea of establishing guidelines for which these machine learning algorithms operate (Bagaric, 2022). A largely accepted definition of fairness in the clinical space is solutions which benefit all racial, ethnic, and gender groups equally. Researchers have proposed several models that could work to better adapt our advanced technologies to that definition of fairness, such as variational autoencoders and adversarial learning models, which attempt to avoid sensitive attributes specific to overrepresented groups. (Mehrabi et al., 2021)  Within this context, researchers have put forth various guidelines to analyze the current medical literature regarding machine learning, after looking at high-impact medical research papers. (Giovanola et al., 2021)
They emphasize the need to describe not only the disease or condition under study, but the ethno racial and gender differences in “disease prevalence, biomarkers, and outcomes.” Additionally, sources of data must be analyzed for their distinction in ethnoracial and explicit reports of the ethno racial background of individuals in the training data set (Pham et al., 2021). The training data must be formatted in such a way that it is easy to compare to census data, in order to prevent the aforementioned shift distribution problem. More broadly, transparency surrounding the gathering of this data must be heavily emphasized, for public and academic confidence (Bagaric et al., 2021; Celi et al., 2022; Challen et al., 2019).
	Researchers suggest that if the medical field adopts the proposed measures, machine learning systems will be able to support medical professionals, and the world at large, without providing unequal benefits to specific groups.
Machine learning systems are indubitably the future of healthcare, but the dangers of bias must be addressed in order for the future to be bright and without unequal outcomes. The research presented has several key limitations, the most apparent of which being the inherent lack of transparency in AI; AI is naturally a black-box phenomenon, so without closely monitored and deliberate guidelines for its creation, we risk developing malicious systems.  I recommend more research into the specific impacts on marginalized communities caused by biased machine learning systems. Currently, researchers have focused on the theoretical issues of these biases, but little research has been done on the current real world effects of these systems. Analyzing machine learning health outcomes by ethnoracial and gender differences will help spur more research into eliminating bias in these systems. This research is complicated by the fact that our non machine-based methods of medicine come with human biases, which makes establishing a baseline quite difficult. Thus, I additionally propose more research into establishing standards for excellence for these AI systems. We must have reliable data on outcomes that are measured by ethnoracial categories in order to eventually hope to eradicate the biases that AI presents. In accordance with that, we must begin labeling all of our medical data and seeking more diverse population groups. The issue of lack of labeling is a large one, and we must provide data on the ethnoracial and gender identities of the patients in training samples, as well as seeking more diverse and representative sample sets, in order for our machine learning systems to become unbiased.
